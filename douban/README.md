# Douban Crawler

关于电视剧的豆瓣爬虫，看剧人必备

## 数据

- 关于剧集 中文名，外文名，图片，系列名称，类别，评分，首播日期，当前季，当季总集数，单集长度，是否为最新季，热度（评分人数），豆瓣链接，imdb链接，简介，是否完结，创建时间，修改时间

- 关于系列

  系列名，国家/地区，媒体，总季数，创建时间，修改时间

- 注意

  id和时间对于没有数据库需求的可忽略

- 数据库

  增加了随机生成id的方法，在将数据存入sql数据库时，可以用自增的行号作主键，生成的id作唯一性约束（没有修改原函数中的uuid，但是不影响使用和修改id）

## 最新 `update_data.py`

将数据写入 `.csv`，应用生成随机id方法，可用于数据库**更新**需要，也可以满足一般爬取需求。

> 推荐使用该方法

## 单页面

- 运行

    - 安装python以及相应的模块

    - 打开豆瓣，打开你想获取的剧集对应的网页，复制链接
    - 运行 `single_page.py` , 输入复制的链接
    - 图片会保存在 `/picture` 下
    - 获取的数据会存在 `/data` 下的 `.txt`
    - 可以将 `.txt` 文件导入到SQL等数据库，如果需要，可自行编写代码，获取数据时直接存储到数据库

- 注意
    - 由于爬取的是单页面数据，所以没有做错误检查，如果报错，则说明网页上有数据不存在，例如有些未播出的剧集，就没有评分等属性
    - 解决方法：注释掉对不存在信息处理的代码，自行添加相关的属性以及值

## 多页爬取

- 如果你有各种代理服务的ip，以及各种浏览器的headers，了解一些反反爬的知识，那么你可以尝试自动爬取多个剧集页面的信息
- 你需要:
    - 修改请求，写入你的各种ip和headers，写一个随机函数随机获取其中之一
    - 写一个txt文件，写入你想获取的剧集名称，系列名即可
    - 遍历你写的txt文件，通过 `get_url(kw)` 函数获取你要的url，并写入 `targe_urls`
    - ~~遍历 `target_urls` ，通过 `get_data(url)` 获取所有数据并存入list `data_v` 和 `data_s` 中~~
    - 遍历 `target_urls` ，通过 `get_data(url)` 获取所有数据并写入指定的 `.txt` 文件
- 注意:
    - 如果缺少反反爬措施，那么也许你刚得到了好几百个url，还没获取多少数据，ip就被ban了（俺就是）
    - 如果要爬取的网页较多，虽然统一写入速度较快，但可能在写入前就发生错误，导致程序停止（主要是由于获取的url有误或有的页面缺少爬取的数据）
